{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# Word vectors (FastText) for Baseline\n\n#### Create Spacy model from word vectors\n\n```bash\npython -m spacy init-model en output/cord19_docrel/spacy/en_cord19_fasttext_300d --vectors-loc output/cord19_docrel/cord19.fasttext.w2v.txt\npython -m spacy init-model en output/acl_docrel/spacy/en_acl_fasttext_300d --vectors-loc output/acl_docrel/acl.fasttext.w2v.txt\n```\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING W\u0026B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from smart_open import open\n",
        "from nlp import load_dataset\n",
        "import nlp\n",
        "import acl.utils\n",
        "from trainer_cli import ExperimentArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## CORD19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "data_dir \u003d Path(\u0027./output/cord19_docrel\u0027)\n",
        "\n",
        "experiment_args \u003d ExperimentArguments(\n",
        "    nlp_dataset\u003d\u0027./datasets/cord19_docrel/cord19_docrel.py\u0027,\n",
        "    nlp_cache_dir\u003d\u0027./data/nlp_cache\u0027,\n",
        "    doc_id_col\u003d\u0027doi\u0027,\n",
        "    doc_a_col\u003d\u0027from_doi\u0027,\n",
        "    doc_b_col\u003d\u0027to_doi\u0027,\n",
        "    cv_fold\u003d1,\n",
        ")\n",
        "\n",
        "docs_ds \u003d load_dataset(experiment_args.nlp_dataset,\n",
        "                       name\u003d\u0027docs\u0027,\n",
        "                       cache_dir\u003dexperiment_args.nlp_cache_dir,\n",
        "                       split\u003dnlp.Split(\u0027docs\u0027))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 16,181,414\n"
          ]
        }
      ],
      "source": [
        "# Extract tokens from each document and create token file.\n",
        "tokens_count \u003d 0\n",
        "with open(data_dir / \u0027tokens.txt\u0027, \u0027w\u0027) as f:\n",
        "    for idx, doc in docs_ds.data.to_pandas().iterrows():\n",
        "        text \u003d acl.utils.get_text_from_doc(doc)  \n",
        "        for token in gensim.utils.simple_preprocess(text, min_len\u003d2, max_len\u003d15):\n",
        "            f.write(token + \u0027 \u0027)\n",
        "            tokens_count +\u003d 1\n",
        "        f.write(\u0027\\n\u0027)\n",
        "print(f\u0027Total tokens: {tokens_count:,}\u0027)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model \u003d fasttext.train_unsupervised(str(data_dir / \u0027tokens.txt\u0027), \n",
        "                                    model\u003d\u0027skipgram\u0027, \n",
        "                                    lr\u003d0.05, # learning rate [0.05]\n",
        "                                    dim\u003d300,   # size of word vectors [100]\n",
        "                                    ws\u003d5,                # size of the context window [5]\n",
        "                                    epoch\u003d5,             # number of epochs [5]\n",
        "                                    thread\u003d4,            # number of threads [number of cpus]\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "model.save_model(str(data_dir / \u0027cord19.fasttext.bin\u0027))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from gensim.models.wrappers import FastText\n",
        "\n",
        "ft_model \u003d FastText.load_fasttext_format(str(data_dir / \u0027cord19.fasttext.bin\u0027))\n",
        "ft_model.wv.save_word2vec_format(data_dir / \u0027cord19.fasttext.w2v.txt\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Unset\n",
        "del ft_model\n",
        "del model\n",
        "del docs_ds\n",
        "del experiment_args\n",
        "del data_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## ACL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset acl_docrel/docs (download: Unknown size, generated: Unknown size, total: Unknown size) to ./data/nlp_cache/acl_docrel/docs/0.1.0...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5212702e85614bef8a3c2add3e36093e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children\u003d(IntProgress(value\u003d0, description\u003d\u0027Downloading\u0027, max\u003d312525939, style\u003dProgressStyle(description_â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children\u003d(IntProgress(value\u003d1, bar_style\u003d\u0027info\u0027, max\u003d1), HTML(value\u003d\u0027\u0027)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r",
            "Dataset acl_docrel downloaded and prepared to ./data/nlp_cache/acl_docrel/docs/0.1.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "data_dir \u003d Path(\u0027./output/acl_docrel\u0027)\n",
        "\n",
        "experiment_args \u003d ExperimentArguments(\n",
        "    nlp_dataset\u003d\u0027./datasets/acl_docrel/acl_docrel.py\u0027,\n",
        "    nlp_cache_dir\u003d\u0027./data/nlp_cache\u0027,\n",
        "    doc_id_col\u003d\u0027s2_id\u0027,\n",
        "    doc_a_col\u003d\u0027from_s2_id\u0027,\n",
        "    doc_b_col\u003d\u0027to_s2_id\u0027,\n",
        "    cv_fold\u003d1,\n",
        ")\n",
        "\n",
        "docs_ds \u003d load_dataset(experiment_args.nlp_dataset,\n",
        "                       name\u003d\u0027docs\u0027,\n",
        "                       cache_dir\u003dexperiment_args.nlp_cache_dir,\n",
        "                       split\u003dnlp.Split(\u0027docs\u0027))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 2,194,010\n"
          ]
        }
      ],
      "source": [
        "# Extract tokens from each document and create token file.\n",
        "tokens_count \u003d 0\n",
        "with open(data_dir / \u0027tokens.txt\u0027, \u0027w\u0027) as f:\n",
        "    for idx, doc in docs_ds.data.to_pandas().iterrows():\n",
        "        text \u003d acl.utils.get_text_from_doc(doc)  \n",
        "        for token in gensim.utils.simple_preprocess(text, min_len\u003d2, max_len\u003d15):\n",
        "            f.write(token + \u0027 \u0027)\n",
        "            tokens_count +\u003d 1\n",
        "        f.write(\u0027\\n\u0027)\n",
        "        \n",
        "# Total tokens: 2,194,010\n",
        "print(f\u0027Total tokens: {tokens_count:,}\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "model \u003d fasttext.train_unsupervised(str(data_dir / \u0027tokens.txt\u0027), \n",
        "                                    model\u003d\u0027skipgram\u0027, \n",
        "                                    lr\u003d0.05, # learning rate [0.05]\n",
        "                                    dim\u003d300,   # size of word vectors [100]\n",
        "                                    ws\u003d5,                # size of the context window [5]\n",
        "                                    epoch\u003d5,             # number of epochs [5]\n",
        "                                    thread\u003d4,            # number of threads [number of cpus]\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "model.save_model(str(data_dir / \u0027acl.fasttext.bin\u0027))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from gensim.models.wrappers import FastText\n",
        "\n",
        "ft_model \u003d FastText.load_fasttext_format(str(data_dir / \u0027acl.fasttext.bin\u0027))\n",
        "ft_model.wv.save_word2vec_format(data_dir / \u0027acl.fasttext.w2v.txt\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:acl-anthology] *",
      "language": "python",
      "name": "conda-env-acl-anthology-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}